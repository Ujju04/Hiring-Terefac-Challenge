{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlBPSPVUQeFF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n"
      ],
      "metadata": {
        "id": "Ygqny4ZGQ_0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility Config\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# CIFAR-10 Normalization\n",
        "\n",
        "norm_mean = (0.4914, 0.4822, 0.4465)\n",
        "norm_std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(norm_mean, norm_std),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(norm_mean, norm_std),\n",
        "])\n"
      ],
      "metadata": {
        "id": "2MlqRXXWyPIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset & Split\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=train_tf\n",
        ")\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_tf\n",
        ")\n",
        "\n",
        "train_len = int(0.8 * len(full_train))\n",
        "val_len = len(full_train) - train_len\n",
        "\n",
        "train_data, val_data = random_split(full_train, [train_len, val_len])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "class_names = full_train.classes\n"
      ],
      "metadata": {
        "id": "tL4EJKlKyD-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SE Block\n",
        "\n",
        "class SEUnit(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        pooled = F.adaptive_avg_pool2d(x, 1).reshape(b, c)\n",
        "        score = torch.sigmoid(self.fc2(F.relu(self.fc1(pooled)))).reshape(b, c, 1, 1)\n",
        "        return x * score\n",
        "\n",
        "\n",
        "# Residual + SE Block\n",
        "\n",
        "class ResSEBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "        self.se = SEUnit(out_ch)\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(out_ch)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = self.bn2(self.conv2(h))\n",
        "        h = self.se(h)\n",
        "        h += self.shortcut(x)\n",
        "        return F.relu(h)"
      ],
      "metadata": {
        "id": "Xf9c-Tyyx_k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 SE-ResNet\n",
        "\n",
        "class SE_ResNet_CIFAR(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ResSEBlock(64, 64),\n",
        "            ResSEBlock(64, 64)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResSEBlock(64, 128, stride=2),\n",
        "            ResSEBlock(128, 128)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResSEBlock(128, 256, stride=2),\n",
        "            ResSEBlock(256, 256)\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "gx_aKRgGx4Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Loss, Optimizer, AMP\n",
        "\n",
        "model = SE_ResNet_CIFAR().to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "kiQGKnccxyrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train / Eval Loop\n",
        "\n",
        "def process_epoch(loader, training=True):\n",
        "    model.train() if training else model.eval()\n",
        "    total, correct, loss_val = 0, 0, 0\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        if training:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        loss_val += loss.item() * targets.size(0)\n",
        "        correct += (outputs.argmax(1) == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "    return loss_val / total, correct / total\n",
        "\n",
        "\n",
        "train_logs, val_logs = [], []\n",
        "\n",
        "for ep in range(1, 181):\n",
        "    tr_loss, tr_acc = process_epoch(train_loader, True)\n",
        "    va_loss, va_acc = process_epoch(val_loader, False)\n",
        "    scheduler.step()\n",
        "\n",
        "    train_logs.append((tr_loss, tr_acc))\n",
        "    val_logs.append((va_loss, va_acc))\n",
        "\n",
        "    if ep % 10 == 0:\n",
        "        print(f\"Epoch {ep:03d} | Train Acc: {tr_acc:.4f} | Val Acc: {va_acc:.4f}\")\n",
        "\n",
        "print(\"\\nTraining Finished.\\n\")"
      ],
      "metadata": {
        "id": "yT9FDvOBxujJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Evaluation\n",
        "\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        preds = model(x).argmax(1).cpu().numpy()\n",
        "        y_pred.extend(preds)\n",
        "        y_true.extend(y.numpy())\n",
        "\n",
        "test_acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "id": "UFI5lxF0xm_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy / Loss Curves\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot([x[1] for x in train_logs], label='Train')\n",
        "plt.plot([x[1] for x in val_logs], label='Val')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot([x[0] for x in train_logs], label='Train')\n",
        "plt.plot([x[0] for x in val_logs], label='Val')\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ezHQhvzhxmZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------\n",
        "# Confusion Matrix\n",
        "# ------------------------------\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "01KZXErwxigM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Per-Class Accuracy\n",
        "# ------------------------------\n",
        "class_correct = np.zeros(10)\n",
        "class_total = np.zeros(10)\n",
        "\n",
        "for i in range(len(y_true)):\n",
        "    class_total[y_true[i]] += 1\n",
        "    if y_true[i] == y_pred[i]:\n",
        "        class_correct[y_true[i]] += 1\n",
        "\n",
        "print(\"\\nPer-Class Accuracy:\")\n",
        "for idx, cls in enumerate(class_names):\n",
        "    print(f\"{cls}: {class_correct[idx] / class_total[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "gEEQB5C4xgY1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}